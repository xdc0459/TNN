// Tencent is pleased to support the open source community by making TNN available.
//
// Copyright (C) 2020 THL A29 Limited, a Tencent company. All rights reserved.
//
// Licensed under the BSD 3-Clause License (the "License"); you may not use this file except
// in compliance with the License. You may obtain a copy of the License at
//
// https://opensource.org/licenses/BSD-3-Clause
//
// Unless required by applicable law or agreed to in writing, software distributed
// under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
// CONDITIONS OF ANY KIND, either express or implied. See the License for the
// specific language governing permissions and limitations under the License.

#include "torch_optimize.h"

#include <torch/csrc/jit/passes/constant_pooling.h>
#include <torch/csrc/jit/passes/dead_code_elimination.h>
#include <torch/csrc/jit/passes/fold_conv_bn.h>
#include <torch/csrc/jit/passes/remove_dropout.h>
#include <torch/csrc/jit/passes/remove_inplace_ops.h>
#include <torch/csrc/jit/passes/remove_mutation.h>
#include <torch/csrc/jit/passes/freeze_module.h>
#include <torch/csrc/jit/passes/lower_tuples.h>
#include <torch/csrc/jit/passes/subgraph_rewrite.h>

namespace torch {
namespace jit {
    int GetMaxBlockSize(Block* block) {
        int max_block_size = 0;
        for (auto it = block->nodes().begin(); it != block->nodes().end();) {
            auto* node = *it;
            it++;
            for (Block* sub_block : node->blocks()) {
                max_block_size = std::max(GetMaxBlockSize(sub_block) + 1, max_block_size);
            }
        }

        return max_block_size;
    }



    // Remove aten::list from graph
    void RemoveList(Block* block) {
        auto check_aten_list_tensor = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::list) {
                return false;
            }

            at::ListTypePtr list_type = node->output(0)->type()->cast<at::ListType>();
            if (list_type && list_type->getElementType()->kind()==at::TypeKind::TensorType) {
                return true;
            }
            return false;
        };

        std::vector<Node*> deleted_nodes;

        for (auto it = block->nodes().begin(); it != block->nodes().end(); it++) {
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                RemoveList(sub_block);
            }
            if (!check_aten_list_tensor(node)) {
                continue;
            }
            node->output(0)->replaceAllUsesWith(node->input(0));
            it->removeInput(0);
            deleted_nodes.push_back(node);
        }

        for (auto del_node : deleted_nodes) {
            del_node->destroy();
        }
    }


    // Prerequisite: RemoveList() must be called first. OP aten::list must not exist in GRAPH.
    // Remove all Tensor[], aka TensorLists in the graph
    // 1. Add prim::ListUnpack after aten::split (with split_sections) if not exist.
    // 2. Store name of Tensors generated by prim::ListUnpack.
    // 3. Remove aten::_set_item, aten::__getitem__ OPs on Tensor Lists because TorchConverter cannot work on such kind of Tensor[].
    // 4. Reconstruct aten::cat() affected by aten::_set_item.
    void UnpackTensorLists(Graph* graph, Block* block) {
        auto check_listunpack = [](torch::jit::Node* node) -> bool {
            if (node->kind() != c10::prim::ListUnpack) {
                return false;
            }
            return true;
        };

        auto check_aten_split_sections_without_listunpack = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::split) {
                return false;
            }
            at::ListTypePtr list_type = node->output(0)->type()->cast<at::ListType>();
            if (!list_type || !toIValue(node->input(1))) {
                return false;
            }
            if (toIValue(node->input(1)).value().isList()) {
                auto split_users = node->output(0)->uses();
                for (int i=0; i<split_users.size(); i++) {
                    if (split_users[i].user->kind()==c10::prim::ListUnpack) {
                        return false;
                    }
                }
            } else {
                // TODO: Only work for split with sections right now. split with split size not supported yet.
                return false;
            }
            return true;
        };

        auto check_aten_setitem = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::_set_item) {
                return false;
            }
            if (at::ListTypePtr list_type = node->input(0)->type()->cast<at::ListType>()) {
                if (list_type->getElementType()->kind()==at::TypeKind::TensorType) {
                    return true;
                }
            }
            return false;
        };

        auto check_aten_getitem = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::__getitem__) {
                return false;
            }
            if (at::ListTypePtr list_type = node->input(0)->type()->cast<at::ListType>()) {
                if (list_type->getElementType()->kind()==at::TypeKind::TensorType) {
                    return true;
                }
            }
            return false;
        };

        auto check_aten_cat_setted_list = [](torch::jit::Node* node, const std::set<std::string>& setted_tensor_lists) -> bool {
            if (node->kind() != at::aten::cat) {
                return false;
            }
            if (setted_tensor_lists.find(node->input(0)->debugName()) != setted_tensor_lists.end()) {
                return true;
            }
            return false;
        };

        std::vector<Node*> deleted_nodes;
        std::set<std::string> setted_tensor_lists;
        std::map<std::string, std::vector<Value*>> tensor_list_nodemap;

        for (auto it = block->nodes().begin(); it != block->nodes().end(); it++) {
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                UnpackTensorLists(graph, sub_block);
            }

            if (!check_listunpack(node) && !check_aten_split_sections_without_listunpack(node) &&
                !check_aten_setitem(node) && !check_aten_getitem(node) &&
                !check_aten_cat_setted_list(node, setted_tensor_lists)) {
                continue;
            }

            if (check_listunpack(node)) {
                // In most cases, there is no need to record existing prim::ListUnpack
                // however, if prim::ListUnpack already exists after aten::split,
                // but aten::_set_item and aten::__getitem__ still use such aten::split ans input.
                // BUG will occur if we do not store info about the existing prim::ListUnpack.
                const int outputs_size = node->outputs().size();
                std::vector<Value*> nodemap;
                for (int i=0; i<outputs_size; i++) {
                    nodemap.push_back(node->output(i));
                }
                tensor_list_nodemap[node->input(0)->debugName()] = nodemap;
            } else if (check_aten_split_sections_without_listunpack(node)){
                int num_split_outputs = TNN_NS::conversion::getValue<std::vector<int64_t>>(node->input(1)).size();
                Node* new_list_unpack_node = graph->createListUnpack(node->output(0), num_split_outputs);
                std::vector<Value*> nodemap;
                for (int i=0; i<num_split_outputs; i++) {
                    nodemap.push_back(new_list_unpack_node->output(i));
                }
                tensor_list_nodemap[node->output(0)->debugName()] = nodemap;
                new_list_unpack_node->insertAfter(node);
            } else if (check_aten_setitem(node)) {
                std::string list_name = node->input(0)->debugName();
                int set_index = static_cast<int>(TNN_NS::conversion::getValue<int64_t>(node->input(1)));
                tensor_list_nodemap[list_name][set_index] = node->input(2);
                deleted_nodes.push_back(node);
                setted_tensor_lists.insert(list_name);
            } else if (check_aten_getitem(node)) {
                std::string list_name = node->input(0)->debugName();
                int get_index = static_cast<int>(TNN_NS::conversion::getValue<int64_t>(node->input(1)));
                Value* target_value = tensor_list_nodemap[list_name][get_index];
                node->output(0)->replaceAllUsesWith(target_value);
                deleted_nodes.push_back(node);
            } else if (check_aten_cat_setted_list(node, setted_tensor_lists)) {
                Node* new_list_construct_node = graph->createList(c10::TensorType::get(), tensor_list_nodemap[node->input(0)->debugName()]);
                new_list_construct_node->insertBefore(node);
                node->replaceInput(0, new_list_construct_node->output(0));
            }
        }

        for (auto del_node : deleted_nodes) {
            std::string out0_name = del_node->output(0)->debugName();
            del_node->destroy();
        }
    }


    void RemoveListAppend(Graph* graph, Block* block) {
        auto check_node = [](torch::jit::Node* node) -> bool {
            if (!(node->kind() == aten::append && node->inputs().at(0)->node()->kind() == prim::ListConstruct)) {
                return false;
            }
            if (node->owningBlock() != node->inputs().at(0)->node()->owningBlock()) {
                return false;
            }

            return true;
        };

        int max_block_size = GetMaxBlockSize(block);

        if (max_block_size > 1) {
            return;
        }

        for (auto it = block->nodes().begin(); it != block->nodes().end();) {
            auto* node = *it;
            it++;

            for (Block* sub_block : node->blocks()) {
                RemoveListAppend(graph, sub_block);
            }

            if (!check_node(node)) {
                continue;
            }

            Value* mutated_value = node->inputs().at(0);
            Node* list_node      = mutated_value->node();
            Node* new_list_node  = graph->create(prim::ListConstruct, 1);
            for (Value* input : list_node->inputs()) {
                new_list_node->addInput(input);
            }
            new_list_node->addInput(node->inputs().at(1));
            new_list_node->copyMetadata(list_node);
            new_list_node->insertAfter(node);
            new_list_node->output()->setType(list_node->output()->type());
            mutated_value->replaceAllUsesAfterNodeWith(node, new_list_node->output());
            node->destroy();
        }
    }


    void LowerGelu(std::shared_ptr<torch::jit::Graph>& graph) {
      std::string gelu_pattern = R"IR(
            graph(%x : Tensor):
                %out : Tensor = aten::gelu(%x)
                return (%out))IR";

      // This gelu_approximate_pattern schema exists in 21.11, 21.12, 22.01 containers of pytorch. These container versions
      // use an unmerged PR in pytorch : https://github.com/pytorch/pytorch/pull/61439. We reduce this to regular Gelu.
      std::string gelu_approximate_pattern = R"IR(
            graph(%x : Tensor, %approx):
                %out : Tensor = aten::gelu(%x, %approx)
                return (%out))IR";

      std::string gelu_reduce_pattern = R"IR(
        graph(%x.1 : Tensor):
            %6 : float = prim::Constant[value=0.044714999999999998]()
            %5 : float = prim::Constant[value=0.79788456080000003]()
            %4 : float = prim::Constant[value=1.]()
            %3 : float = prim::Constant[value=0.5]()
            %2 : int = prim::Constant[value=1]()
            %7 : Tensor = aten::mul(%x.1, %3)
            %8 : Tensor = aten::mul(%x.1, %5)
            %9 : Tensor = aten::mul(%x.1, %6)
            %10 : Tensor = aten::mul(%9, %x.1)
            %11 : Tensor = aten::add(%10, %4, %2)
            %12 : Tensor = aten::mul(%8, %11)
            %13 : Tensor = aten::tanh(%12)
            %14 : Tensor = aten::add(%13, %4, %2)
            %15 : Tensor = aten::mul(%7, %14)
            return (%15))IR";

      // This is same as gelu_reduce_pattern except for an additional input %approx.
      // SubgraphRewriter only works as expected if the number of inputs to gelu_approximate_pattern
      // and gelu_reduce_multi_input_pattern are same.
      std::string gelu_reduce_multi_input_pattern = R"IR(
        graph(%x.1 : Tensor, %approx):
            %6 : float = prim::Constant[value=0.044714999999999998]()
            %5 : float = prim::Constant[value=0.79788456080000003]()
            %4 : float = prim::Constant[value=1.]()
            %3 : float = prim::Constant[value=0.5]()
            %2 : int = prim::Constant[value=1]()
            %7 : Tensor = aten::mul(%x.1, %3)
            %8 : Tensor = aten::mul(%x.1, %5)
            %9 : Tensor = aten::mul(%x.1, %6)
            %10 : Tensor = aten::mul(%9, %x.1)
            %11 : Tensor = aten::add(%10, %4, %2)
            %12 : Tensor = aten::mul(%8, %11)
            %13 : Tensor = aten::tanh(%12)
            %14 : Tensor = aten::add(%13, %4, %2)
            %15 : Tensor = aten::mul(%7, %14)
            return (%15))IR";

      // replace aten::gelu with pointwise operations
      torch::jit::SubgraphRewriter map_gelu_to_pointwise_ops;
      map_gelu_to_pointwise_ops.RegisterRewritePattern(gelu_pattern, gelu_reduce_pattern);
      map_gelu_to_pointwise_ops.runOnGraph(graph);

      torch::jit::SubgraphRewriter map_gelu_approximate_to_pointwise_ops;
      map_gelu_approximate_to_pointwise_ops.RegisterRewritePattern(
          gelu_approximate_pattern, gelu_reduce_multi_input_pattern);
      map_gelu_approximate_to_pointwise_ops.runOnGraph(graph);

    }


    void RemoveConcat(Block* block) {
        auto check_node = [](torch::jit::Node* node) -> bool {
            if (node->kind() != at::aten::cat) {
                return false;
            }
            if (node->inputs()[0]->node()->inputs().size() != 1) {
                return false;
            }

            return true;
        };

        std::vector<Node*> deleted_nodes;

        for (auto it = block->nodes().rbegin(); it != block->nodes().rend(); it++) {
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                RemoveConcat(sub_block);
            }

            if (!check_node(node)) {
                continue;
            }

            Value* input_value  = node->inputs()[0]->node()->inputs()[0];
            Value* output_value = node->outputs()[0];
            output_value->replaceAllUsesWith(input_value);
            deleted_nodes.push_back(node);
        }

        for (auto del_node : deleted_nodes) {
            del_node->destroy();
        }
    }

    void RemoveNoneTypeFromTuple(Block* block) {
        for (auto it = block->nodes().rbegin(); it != block->nodes().rend(); it++) {
            std::vector<size_t> deleted_inputs_index;
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                RemoveNoneTypeFromTuple(sub_block);
            }
            if (node->kind() != at::prim::TupleConstruct) {
                continue;
            }
            const int inputs_size = node->inputs().size();
            for (int i = 0; i < inputs_size; i++) {
                if (node->inputs()[i]->type()->kind() == c10::TypeKind::NoneType) {
                    deleted_inputs_index.push_back(i);
                }
            }
            for (const auto& index : deleted_inputs_index) {
                node->removeInput(index);
            }
        }
    }

    void RemoveException(torch::jit::Block* block) {
        auto check_node = [](torch::jit::Node* n) -> bool {
            if (n->blocks().size() != 2) {
                return false;
            }
            auto block0 = n->blocks()[0];
            auto block1 = n->blocks()[1];
            if (block0->outputs().size() != 0 || block1->outputs().size() != 0) {
                // Make sure that the node doesn't actually produce any Value that are
                // used by other nodes
                return false;
            }

            auto block0_start = block0->nodes().begin();
            auto block1_start = block1->nodes().begin();

            // Make sure that there is at least one empty block
            if (block0_start->kind() != prim::Return && block1_start->kind() != prim::Return) {
                return false;
            }

            if ((*block1_start)->kind() == prim::Return) {
                if ((*block0_start)->kind() == prim::RaiseException) {
                    if ((*(++block0_start))->kind() == prim::Return) {
                        // Make sure that block0 is solely just the exception and the return
                        return true;
                    }
                } else if ((*block0_start)->kind() == aten::format &&
                           (*(++block0_start))->kind() == prim::RaiseException) {
                    if ((*(++block0_start))->kind() == prim::Return) {
                        // Make sure that block0 is solely just the exception and the return
                        return true;
                    }
                }
            }

            if ((*block0_start)->kind() == prim::Return) {
                if ((*block1_start)->kind() == prim::RaiseException) {
                    if ((*(++block1_start))->kind() == prim::Return) {
                        // Make sure that block0 is solely just the exception and the return
                        return true;
                    }
                } else if ((*block1_start)->kind() == aten::format &&
                           (*(++block1_start))->kind() == prim::RaiseException) {
                    if ((*(++block1_start))->kind() == prim::Return) {
                        // Make sure that block0 is solely just the exception and the return
                        return true;
                    }
                }
            }

            return false;
        };

        for (auto it = block->nodes().begin(), end = block->nodes().end(); it != end; ++it) {
            for (auto b : it->blocks()) {
                RemoveException(b);
            }

            if (it->kind() == prim::If && check_node(*it)) {
                it.destroyCurrent();
            }
        }
    }

    void RemoveSlice(Block* block) {
        std::vector<Node*> deleted_nodes;

        for (auto it = block->nodes().rbegin(); it != block->nodes().rend(); it++) {
            Node* node = *it;
            for (auto sub_block : node->blocks()) {
                RemoveSlice(sub_block);
            }

            if (node->kind() != at::aten::slice) {
                continue;
            }

            const auto& inputs = node->inputs();
            const auto dim     = TNN_NS::conversion::getValue<int64_t>(inputs[1]);
            const auto start   = TNN_NS::conversion::getValue<int64_t>(inputs[2]);
            const auto end     = TNN_NS::conversion::getValue<int64_t>(inputs[3]);
            const auto step    = TNN_NS::conversion::getValue<int64_t>(inputs[4]);
            if (dim != 0 || start != 0 || step != 1 || end != LONG_LONG_MAX) {
                continue;
            }

            Value* input_value  = node->inputs()[0];
            Value* output_value = node->outputs()[0];
            output_value->replaceAllUsesWith(input_value);
            deleted_nodes.push_back(node);
        }

        for (auto del_node : deleted_nodes) {
            del_node->destroy();
        }
    }

    void RemoveClone(Block* block) {
        std::vector<Node*> deleted_nodes;

        for (auto it = block->nodes().rbegin(); it != block->nodes().rend(); it++) {
            Node* node = *it;
            for (auto block : node->blocks()) {
                RemoveClone(block);
            }
            if ((node->kind() == c10::Symbol::fromQualString("aten::clone"))) {
                Value* input_value = node->inputs()[0];
                Value* output_value = node->outputs()[0];
                output_value->replaceAllUsesWith(input_value);
                deleted_nodes.push_back(node);
            }
        }
        for (auto del_node : deleted_nodes) {
            del_node->destroy();
        }
    }

    void RemoveContiguous(std::shared_ptr<Graph> graph) {
        std::string contiguous_pattern    = R"IR(
        graph(%input, %1):
            %2 = aten::contiguous(%input, %1)
            return (%2))IR";
        std::string no_contiguous_pattern = R"IR(
        graph(%input, %1):
            return (%input))IR";

        // remove contiguous
        torch::jit::SubgraphRewriter remove_contiguous;
        remove_contiguous.RegisterRewritePattern(contiguous_pattern, no_contiguous_pattern);
        remove_contiguous.runOnGraph(graph);
    }


    // Partly learnt from torch/csrc/jit/passes/lower_graph.cpp
    // torch::jit::LowerGraph() Pass removes GetAttr but meanwhile add Constant Values to inputs.
    // We don't need such kind of Inputs,
    // Instead, We remove prim::GetAttr from Graph, replace part of "GetAttr" with Constant Values.
    void LowerGetAttr(std::shared_ptr<Graph> graph, const c10::intrusive_ptr<c10::ivalue::Object>& module) {
        struct Slot {
            c10::intrusive_ptr<c10::ivalue::Object> obj;
            size_t offset;
            bool operator==(const Slot& other) const {
                return (this->obj == other.obj && this->offset == other.offset);
            }
        };
        struct SlotHash {
            std::size_t operator()(const Slot& slot) const {
                auto obj_hash = std::hash<c10::ivalue::Object*>{}(slot.obj.get());
                auto offset_hash = std::hash<size_t>{}(slot.offset);
                return c10::hash_combine(obj_hash, offset_hash);
            }
        };
        std::unordered_map<Slot, size_t, SlotHash> slot_to_offset;

        struct ToScan {
            c10::intrusive_ptr<c10::ivalue::Object> mod;
            torch::jit::Node* n;
            size_t offset;
        };

        // Find out the first constant node, all inserted constant nodes are moved
        // before the first constant node.
        torch::jit::Node* first_constant_node = nullptr;
        for (auto it = graph->nodes().begin(); it != graph->nodes().end(); it++) {
            Node* node = *it;
            if (node->kind() == at::prim::Constant) {
                first_constant_node = node;
                break;
            }
        }
        // MayBe Needed, Inline to remove method/function calls
        //torch::jit::Inline(*graph);

        std::vector<torch::jit::Value*> extra_value_constants;
        std::vector<ToScan> to_scan;
        std::vector<torch::jit::Node*> to_clean;  // Nodes that should be dead at the end.

        auto getOrAddSlot = [&](const Slot& slot) -> Value* {
            auto it = slot_to_offset.find(slot);
            if (it != slot_to_offset.end()) {
                return extra_value_constants[it->second];
            }
            torch::jit::Value* const_value_inserted = graph->insertConstant(slot.obj->getSlot(slot.offset));
            torch::jit::Node* const_node_inserted = const_value_inserted->node();
            const_node_inserted->moveBefore(first_constant_node);
            extra_value_constants.push_back(const_value_inserted);
            slot_to_offset[slot] = extra_value_constants.size() - 1;
            return const_value_inserted;
        };

        // Get the first input of Graph, usually, %self.1
        auto self_value = graph->inputs().at(0);
        for (auto use : self_value->uses()) {
            to_scan.emplace_back(ToScan{module, use.user, use.offset});
        }
        
        while (to_scan.size()>0) {
            auto e = to_scan.back();
            to_scan.pop_back();
            
            // TODO: DEAL WITH prim::fork in the future.
            // when we lambda lift forks, first-class modules may be passed across
            // forks. This code recursively lowers the module in the fork call.
            //if (e.n->kind() == at::prim::fork) {
            //    auto subgraph = e.n->g(c10::attr::Subgraph);
            //    std::vector<Slot> new_slots;
            //    std::tie(subgraph, new_slots) = lower_graph(e.mod, *subgraph, e.offset);
            //    e.n->g_(attr::Subgraph, subgraph);
            //    for (const Slot& slot : new_slots) {
            //        e.n->addInput(getOrAddSlot(slot));
            //    }
            //    e.n->removeInput(e.offset);
            //    continue;
            //}
            if (e.n->kind() == prim::PythonOp) {
                LOGE("Couldn't export Python method.");
                break;
            }

            if (e.n->kind() != c10::prim::GetAttr) {
                LOGE("LowerGetAttr Got ERROR: user of %self.1 should be prim::GetAttr of prim::Fork!");
                break;
            }
            
            size_t slot_idx = e.mod->type()->getAttributeSlot(e.n->s(c10::attr::name));
            auto iv = e.mod->getSlot(slot_idx);
            if (c10::ClassTypePtr c = e.n->output()->type()->cast<c10::ClassType>()) {
                if (c->is_module()) {
                    for (auto use : e.n->output()->uses()) {
                        to_scan.emplace_back(ToScan{iv.toObject(), use.user, use.offset});
                    }
                    to_clean.emplace_back(e.n);
                    continue;
                }
            }
            e.n->output()->replaceAllUsesWith(getOrAddSlot({e.mod, slot_idx}));
            e.n->destroy();
        }

        while (to_clean.size() > 0) {
            torch::jit::Node* n = to_clean.back();
            if (n->hasUses()) {
                LOGE("LowerGetAttr Got ERROR: to_clean Value still has uses.!");
            }
            n->destroy();
            to_clean.pop_back();
        }

        if (self_value->hasUses()) {
            LOGE("LowerGetAttr Got ERROR: After LowerGetAttr, input %self.1 still has uses.!");
        }
    }

    // DEBUG, add output to Graph
    void DebugAddOutput(Graph* graph) {
        int curr_idx = 0;
        auto block = graph->block();
       
        // When Output is a single Tensor.
        // by name and type
        std::string target_out_name = "x.38";
        for (auto it = block->nodes().begin(); it != block->nodes().end(); it++) {
            Node* node = *it;
            if (node->inputs().size()>0 && node->outputs().size()>0 &&
                node->output(0)->debugName()==target_out_name) {
                graph->eraseOutput(0);
                graph->registerOutput(node->output(0));
                std::cout << "=== [Graph DEBUG] register out0 of [" << curr_idx << "]th node to output. ";
                std::cout << "Target node type = " << node->kind().toQualString() << ", in0.name=" << node->input(0)->debugName();
                std::cout << ", out0.name=" << node->output(0)->debugName() << " ==="<< std::endl;
                break;
            }
            curr_idx++;
        }

        // When Output is a Tensor tuple.
        /*
        std::vector<Value*> out_tuple_inputs;
        for (auto it = block->nodes().begin(); it != block->nodes().end(); it++) {
            Node* node = *it;
            if (node->kind()==c10::prim::TupleConstruct) {
                std::cout << "=== [Graph DEBUG] c10::prim::TupleConstruct, ";
                std::cout << ", in0.name=" << node->input(0)->debugName();
                std::cout << ", out0.name=" << node->output(0)->debugName() << " ==="<< std::endl;
                for (int i=0; i<node->inputs().size(); i++) {
                    out_tuple_inputs.push_back(node->input(i));
                }
            }
        }
        // by name and type
        //std::string target_out_name = "attn_weights3.2";
        //std::string target_out_name = "attn_weights2.2";
        std::string target_out_name = "encoder_padding_mask.1";
        //std::string target_out_name = "x0.15";
        //std::string target_out_name = "xy_matmul.3";
        for (auto it = block->nodes().begin(); it != block->nodes().end(); it++) {
            Node* node = *it;
            if (node->inputs().size()>0 && node->outputs().size()>0 &&
                node->output(0)->debugName()==target_out_name) {
                
                out_tuple_inputs[0] = node->output(0);
                Node* new_tuple_construct_node = graph->createTuple(out_tuple_inputs);
                graph->registerOutput(new_tuple_construct_node->output(0));
                graph->insertNode(new_tuple_construct_node);
                graph->eraseOutput(0);
                std::cout << "=== [Graph DEBUG] register out0 of [" << curr_idx << "]th node to output. ";
                std::cout << "Target node type = " << node->kind().toQualString() << ", in0.name=" << node->input(0)->debugName();
                std::cout << ", out0.name=" << node->output(0)->debugName() << " ==="<< std::endl;
                break;
            }
            curr_idx++;
        }
        */
    }




    void TorchOptPass(script::Module& module) {

        module.eval();
        module = torch::jit::freeze_module(module);
        auto graph = module.get_method("forward").graph();
        LowerSimpleTuples(graph);

        removeDropout(module);
        RemoveException(graph->block());
        RemoveListAppend(graph.get(), graph->block());
//      Remove Concat cause cascade rcnn crash        
//      RemoveConcat(graph->block());
//      RemoveContiguous(graph);

        RemoveList(graph->block());
        UnpackTensorLists(graph.get(), graph->block()); // Call to RemoveList() Required Before Call to UnpackTensorList()!
        LowerGelu(graph);
        LowerGetAttr(graph, module._ivalue());
//      RemoveClone(graph->block());
//      RemoveNoneTypeFromTuple(graph->block());
//      RemoveSlice(graph->block());

        torch::jit::EliminateDeadCode(graph);
        // DEBUG
        //DebugAddOutput(graph.get());
    }
}  // namespace jit
}  // namespace torch
